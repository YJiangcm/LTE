{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1').to('cuda')\n",
    "\n",
    "### zsre\n",
    "data_path=\"data/KnowEdit/ZsRE/zsre_mend_train_10000.json\"\n",
    "with open(data_path, 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "sentences = []\n",
    "subjects = []\n",
    "for i, train_data in enumerate(input_data):\n",
    "    new_fact = train_data['src'] + ' ' + train_data['alt']\n",
    "    sentences.append(new_fact)\n",
    "    subjects.append(train_data['subject'])\n",
    "embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "with open(data_path.split('.')[0] + '_embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'subjects': subjects, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "### wiki_counterfact\n",
    "data_path=\"data/KnowEdit/wiki_counterfact/train_cf.json\"\n",
    "with open(data_path, 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "sentences = []\n",
    "subjects = []\n",
    "for i, train_data in enumerate(input_data):\n",
    "    new_fact = train_data['prompt'] + ' ' + train_data['target_new']\n",
    "    sentences.append(new_fact)\n",
    "    subjects.append(train_data['subject'])\n",
    "embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "with open(data_path.split('.')[0] + '_embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'subjects': subjects, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "### wiki_recent\n",
    "data_path=\"data/KnowEdit/wiki_recent/recent_train.json\"\n",
    "with open(data_path, 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "sentences = []\n",
    "subjects = []\n",
    "for i, train_data in enumerate(input_data):\n",
    "    new_fact = train_data['prompt'] + ' ' + train_data['target_new']\n",
    "    sentences.append(new_fact)\n",
    "    subjects.append(train_data['subject'])\n",
    "embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "with open(data_path.split('.')[0] + '_embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'subjects': subjects, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "### wikibio\n",
    "data_path=\"data/KnowEdit/WikiBio/wikibio-train-all.json\"\n",
    "with open(data_path, 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "sentences = []\n",
    "subjects = []\n",
    "for i, train_data in enumerate(input_data):\n",
    "    new_fact = train_data['labels'] + ' ' + train_data['text']\n",
    "    sentences.append(new_fact)\n",
    "    subjects.append(train_data['concept'])\n",
    "embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "with open(data_path.split('.')[0] + '_embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'subjects': subjects, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "### wiki_counterfact\n",
    "with open(\"data/KnowEdit/wiki_counterfact/train_cf.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    input_data[i]['attribute_len'] = len(input_data[i]['portability'].keys()) + len(input_data[i]['locality'].keys())\n",
    "sorted_input_data = sorted(input_data, key=lambda x: x['attribute_len'], reverse=True)\n",
    "\n",
    "json_str = json.dumps(sorted_input_data, indent=4)\n",
    "with open('data/KnowEdit/wiki_counterfact/train_cf_sorted.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)\n",
    "\n",
    "\n",
    "### wiki_recent\n",
    "with open(\"data/KnowEdit/wiki_recent/recent_train.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    input_data[i]['attribute_len'] = len(input_data[i]['portability'].keys()) + len(input_data[i]['locality'].keys())\n",
    "sorted_input_data = sorted(input_data, key=lambda x: x['attribute_len'], reverse=True)\n",
    "\n",
    "json_str = json.dumps(sorted_input_data, indent=4)\n",
    "with open('data/KnowEdit/wiki_recent/recent_train_sorted.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from sentence_transformers import util\n",
    "\n",
    "def knowledge_edit_template(new_facts, question):\n",
    "    return \"Please acknowledge the updated information provided below and respond to the subsequent question.\\n\\n[Updated Information]:\\n\" \\\n",
    "        + new_facts + \"\\n\\n[Question]:\\n\" + question\n",
    "\n",
    "def sentence_completion_prompt(question):\n",
    "    return f\"Please complete the sentence below. You should ONLY output the completed part.\\n\\n{question}\"\n",
    "\n",
    "def text_completion_prompt(question):\n",
    "    return f\"Please complete the text below. You should ONLY output the completed part.\\n\\n{question}\"\n",
    "\n",
    "def question_answering_prompt(question):\n",
    "    return f\"Please answer the question below. You should ONLY output the answer.\\n\\n{question}\"\n",
    "\n",
    "def data_append(data, source, idx, new_facts, question, answer):\n",
    "    if new_facts:\n",
    "        data.append({\n",
    "            \"id\": \"identity_{0}_{1}\".format(str(idx), source),\n",
    "            \"conversations\": [\n",
    "            {\n",
    "                \"from\": 'human',\n",
    "                \"value\": knowledge_edit_template(new_facts, question)\n",
    "            },\n",
    "            {\n",
    "                \"from\": 'gpt',\n",
    "                \"value\": answer\n",
    "            },\n",
    "            ]\n",
    "        })\n",
    "    else:\n",
    "        data.append({\n",
    "            \"id\": \"identity_{0}_{1}\".format(str(idx), source),\n",
    "            \"conversations\": [\n",
    "            {\n",
    "                \"from\": 'human',\n",
    "                \"value\": question\n",
    "            },\n",
    "            {\n",
    "                \"from\": 'gpt',\n",
    "                \"value\": answer\n",
    "            },\n",
    "            ]\n",
    "        })\n",
    "    idx += 1\n",
    "    return idx, data\n",
    "\n",
    "\n",
    "def retrieve_new_facts(embedding_path, sentence_model, query_sentence, query_subject, num):\n",
    "    with open(embedding_path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_sentences = stored_data['sentences']\n",
    "        stored_subjects = stored_data['subjects']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "\n",
    "    stored_embeddings = torch.tensor(stored_embeddings).to('cuda')\n",
    "    stored_embeddings = util.normalize_embeddings(stored_embeddings)\n",
    "\n",
    "    query_embedding = util.normalize_embeddings(torch.tensor(sentence_model.encode(\n",
    "        query_sentence, show_progress_bar=False)).unsqueeze(0).to('cuda'))\n",
    "\n",
    "    hits = util.semantic_search(query_embedding, stored_embeddings, score_function=util.dot_score, top_k=5)\n",
    "    assert len(hits) == 1\n",
    "    hit = hits[0]\n",
    "    retrieved_sentences = [stored_sentences[hit[k][\"corpus_id\"]] for k in range(len(hit))]\n",
    "    retrieved_subjects = [stored_subjects[hit[k][\"corpus_id\"]] for k in range(len(hit))]\n",
    "\n",
    "    retrieved_sent = []\n",
    "    for i in range(len(retrieved_sentences)):\n",
    "        if retrieved_subjects[i] != query_subject and retrieved_sentences[i] != query_sentence:\n",
    "            retrieved_sent.append(retrieved_sentences[i])\n",
    "\n",
    "    try:        \n",
    "        retrieved_sent = random.sample(retrieved_sent, num)\n",
    "        idx = 1\n",
    "        new_facts = f\"{idx}. \" + query_sentence\n",
    "        for s in retrieved_sent:\n",
    "            idx += 1\n",
    "            new_facts += f\"\\n{idx}. \" + s\n",
    "    except:\n",
    "        new_facts = query_sentence\n",
    "        \n",
    "    return new_facts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out_of_scope_questions = []\n",
    "data = []\n",
    "need_gpt4 = []\n",
    "\n",
    "\n",
    "### ZsRE_train\n",
    "idx = 0\n",
    "with open(\"data/KnowEdit/ZsRE/zsre_mend_train_10000.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    if idx >= 4000:\n",
    "        break\n",
    "    new_facts = input_data[i]['src'] + \" \" + input_data[i]['alt']\n",
    "    subject = input_data[i]['subject']\n",
    "\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.5:\n",
    "        new_facts = new_facts\n",
    "    elif rand_num < 0.75:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/ZsRE/zsre_mend_train_10000_embeddings.pkl', sentence_model, new_facts, subject, num=1)\n",
    "    else:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/ZsRE/zsre_mend_train_10000_embeddings.pkl', sentence_model, new_facts, subject, num=2)\n",
    "\n",
    "    question = input_data[i]['src']\n",
    "    answer = input_data[i]['alt']\n",
    "    ground_truth = input_data[i]['answers'][0] if input_data[i]['answers'] else \"\"\n",
    "\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.3:\n",
    "        # rewrite\n",
    "        if question != \"\" and answer != \"\" and ground_truth != \"\":\n",
    "            idx, data = data_append(data, 'zsre_train', idx, new_facts, question, answer)\n",
    "            idx, data = data_append(data, 'zsre_train', idx, None, question, ground_truth)\n",
    "\n",
    "    else:\n",
    "        # rephrase\n",
    "        question = input_data[i]['rephrase']\n",
    "        if question != \"\" and answer != \"\" and ground_truth != \"\":\n",
    "            idx, data = data_append(data, 'zsre_train', idx, new_facts, question, answer)\n",
    "            idx, data = data_append(data, 'zsre_train', idx, None, question, ground_truth)\n",
    "\n",
    "    question = input_data[i]['loc'].replace(\"nq question: \", \"\")\n",
    "    answer = input_data[i]['loc_ans']\n",
    "    if question != \"\" and answer != \"\":\n",
    "        idx, data = data_append(data, 'zsre_train', idx, new_facts, question, answer)\n",
    "        idx, data = data_append(data, 'zsre_train', idx, None, question, answer)\n",
    "        out_of_scope_questions.append({'question': question, 'answer': answer})\n",
    "\n",
    "\n",
    "### wiki_counterfact_train\n",
    "idx = 0\n",
    "with_in = 0\n",
    "with_out = 0\n",
    "with open(\"data/KnowEdit/wiki_counterfact/train_cf_sorted.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    if with_in == with_out and with_in > 1200:\n",
    "        break\n",
    "    new_facts = input_data[i]['prompt'] + \" \" + input_data[i]['target_new']\n",
    "    subject = input_data[i]['subject']\n",
    "    question = input_data[i]['prompt']\n",
    "    answer = input_data[i]['target_new']\n",
    "    ground_truth = input_data[i]['ground_truth']\n",
    "\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.5:\n",
    "        new_facts = new_facts\n",
    "    elif rand_num < 0.75:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/wiki_counterfact/train_cf_embeddings.pkl', sentence_model, new_facts, subject, num=1)\n",
    "    else:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/wiki_counterfact/train_cf_embeddings.pkl', sentence_model, new_facts, subject, num=2)\n",
    "\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.3:\n",
    "        if question != \"\" and answer != \"\" and ground_truth != \"\":\n",
    "            idx, data = data_append(data, 'wiki_counterfact_train', idx, new_facts, question, answer)\n",
    "            idx, data = data_append(data, 'wiki_counterfact_train', idx, None, question, ground_truth)\n",
    "            with_in += 1\n",
    "    else:\n",
    "        for attribution in ['portability']:\n",
    "            if attribution in input_data[i]:\n",
    "                for k in input_data[i][attribution].keys():\n",
    "                    for j in range(len(input_data[i][attribution][k])):\n",
    "                        question = input_data[i][attribution][k][j]['prompt']\n",
    "                        answer = input_data[i][attribution][k][j]['ground_truth']\n",
    "                        if question != \"\" and answer != \"\":\n",
    "                            idx, data = data_append(data, 'wiki_counterfact_train', idx, new_facts, question, answer)\n",
    "                            idx, data = data_append(data, 'wiki_counterfact_train', idx, None, question, None)\n",
    "                            need_gpt4.append({'question': question, 'prompt_new': sentence_completion_prompt(question)})\n",
    "                            with_in += 1\n",
    "\n",
    "    for attribution in ['locality']:\n",
    "        if attribution in input_data[i]:\n",
    "            for k in input_data[i][attribution].keys():\n",
    "                for j in range(len(input_data[i][attribution][k])):\n",
    "                    question = input_data[i][attribution][k][j]['prompt']\n",
    "                    answer = input_data[i][attribution][k][j]['ground_truth']\n",
    "                    if question != \"\" and answer != \"\":\n",
    "                        idx, data = data_append(data, 'wiki_counterfact_train', idx, new_facts, question, answer)\n",
    "                        idx, data = data_append(data, 'wiki_counterfact_train', idx, None, question, answer)\n",
    "                        with_out += 1\n",
    "                        out_of_scope_questions.append({'question': question, 'answer': answer})\n",
    "\n",
    "### wiki_recent_train\n",
    "idx = 0\n",
    "with_in = 0\n",
    "with_out = 0\n",
    "with open(\"data/KnowEdit/wiki_recent/recent_train_sorted.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    if with_in == with_out and with_in > 1200:\n",
    "        break\n",
    "    new_facts = input_data[i]['prompt'] + \" \" + input_data[i]['target_new']\n",
    "    subject = input_data[i]['subject']\n",
    "    question = input_data[i]['prompt']\n",
    "    answer = input_data[i]['target_new']\n",
    "\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.5:\n",
    "        new_facts = new_facts\n",
    "    elif rand_num < 0.75:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/wiki_recent/recent_train_embeddings.pkl', sentence_model, new_facts, subject, num=1)\n",
    "    else:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/wiki_recent/recent_train_embeddings.pkl', sentence_model, new_facts, subject, num=2)\n",
    "\n",
    "    if question != \"\" and answer != \"\":\n",
    "        idx, data = data_append(data, 'wiki_recent_train', idx, new_facts, question, answer)\n",
    "        idx, data = data_append(data, 'wiki_recent_train', idx, None, question, None)\n",
    "        need_gpt4.append({'question': question, 'prompt_new': sentence_completion_prompt(question)})\n",
    "        with_in += 1\n",
    "\n",
    "    for attribution in ['portability']:\n",
    "        if attribution in input_data[i]:\n",
    "            for k in input_data[i][attribution].keys():\n",
    "                for j in range(len(input_data[i][attribution][k])):\n",
    "                    question = input_data[i][attribution][k][j]['prompt']\n",
    "                    if question != \"\" and [item for sublist in input_data[i][attribution][k][j]['ground_truth'] for item in sublist] != []:\n",
    "                        answer = [item for sublist in input_data[i][attribution][k][j]['ground_truth'] for item in sublist][0]\n",
    "                        idx, data = data_append(data, 'wiki_recent_train', idx, new_facts, question, answer)\n",
    "                        idx, data = data_append(data, 'wiki_recent_train', idx, None, question, None)\n",
    "                        need_gpt4.append({'question': question, 'prompt_new': sentence_completion_prompt(question)})\n",
    "                        with_in += 1\n",
    "    \n",
    "    for attribution in ['locality']:\n",
    "        if attribution in input_data[i]:\n",
    "            for k in input_data[i][attribution].keys():\n",
    "                for j in range(min(len(input_data[i][attribution][k]), 3)):\n",
    "                    question = input_data[i][attribution][k][j]['prompt']\n",
    "                    if question != \"\" and [item for sublist in input_data[i][attribution][k][j]['ground_truth'] for item in sublist] != []:\n",
    "                        answer = [item for sublist in input_data[i][attribution][k][j]['ground_truth'] for item in sublist][0]\n",
    "                        idx, data = data_append(data, 'wiki_recent_train', idx, new_facts, question, answer)\n",
    "                        idx, data = data_append(data, 'wiki_recent_train', idx, None, question, answer)\n",
    "                        with_out += 1\n",
    "                        out_of_scope_questions.append({'question': question, 'answer': answer})\n",
    "\n",
    "\n",
    "### wikibio_train\n",
    "idx = 0\n",
    "with open(\"data/KnowEdit/WikiBio/wikibio-train-all.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(250):\n",
    "    new_facts = input_data[i]['text'] + \" \" + input_data[i]['labels']\n",
    "    subject = input_data[i]['concept']\n",
    "    question = input_data[i]['text']\n",
    "    answer = input_data[i]['labels']\n",
    "\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.5:\n",
    "        new_facts = new_facts\n",
    "    elif rand_num < 0.75:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/WikiBio/wikibio-train-all_embeddings.pkl', sentence_model, new_facts, subject, num=1)\n",
    "    else:\n",
    "        new_facts = retrieve_new_facts('data/KnowEdit/WikiBio/wikibio-train-all_embeddings.pkl', sentence_model, new_facts, subject, num=2)\n",
    "\n",
    "    idx, data = data_append(data, 'wikibio_train', idx, new_facts, question, answer)\n",
    "    idx, data = data_append(data, 'wikibio_train', idx, None, question, None)\n",
    "    need_gpt4.append({'question': question, 'prompt_new': text_completion_prompt(question)})\n",
    "    \n",
    "    for attribution in ['locality']:\n",
    "        if attribution in input_data[i]:\n",
    "            for k in input_data[i][attribution].keys():\n",
    "                for j in range(min(len(input_data[i][attribution][k]), 1)):\n",
    "                    question = input_data[i][attribution][k][j]['prompt']\n",
    "                    if question != \"\" and input_data[i][attribution][k][j]['ground_truth'] != []:\n",
    "                        answer = input_data[i][attribution][k][j]['ground_truth'][0]\n",
    "                        idx, data = data_append(data, 'wikibio_train', idx, new_facts, question, answer)\n",
    "                        idx, data = data_append(data, 'wikibio_train', idx, None, question, answer)\n",
    "                        out_of_scope_questions.append({'question': question, 'answer': answer})\n",
    "\n",
    "\n",
    "### MQUAKE-CF\n",
    "idx = 0\n",
    "with open(\"data/WizardLM/WizardLM_evol_instruct_V2_143k.json\", 'r', encoding='utf-8') as input_file:\n",
    "    wizardlm_data = json.load(input_file)\n",
    "\n",
    "with open(\"data/MQuAKE/MQuAKE-CF.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(2500):\n",
    "    # with edit\n",
    "    requested_rewrite = input_data[i]['requested_rewrite']\n",
    "    new_facts = \"\"\n",
    "    if len(requested_rewrite) > 1:\n",
    "        for j in range(len(requested_rewrite)):\n",
    "            new_facts += str(j+1) + \". \" + requested_rewrite[j]['prompt'].replace(\"{}\", requested_rewrite[j]['subject']) + \" \" + requested_rewrite[j]['target_new']['str'] + \"\\n\"\n",
    "    else:\n",
    "        new_facts += requested_rewrite[0]['prompt'].replace(\"{}\", requested_rewrite[0]['subject']) + \" \" + requested_rewrite[0]['target_new']['str']\n",
    "    new_facts = new_facts.strip(\"\\n\")\n",
    "    question = input_data[i]['questions'][0]\n",
    "\n",
    "    answer = input_data[i]['new_answer'] + \"\\n\"\n",
    "    for k in range(len(input_data[i]['new_single_hops'])):\n",
    "        answer += input_data[i]['new_single_hops'][k]['cloze'] + \" \" + input_data[i]['new_single_hops'][k]['answer'] + \". \"\n",
    "    answer = answer.strip()\n",
    "\n",
    "    idx, data = data_append(data, 'mquake_cf', idx, new_facts, question, answer)\n",
    "\n",
    "    # without edit\n",
    "    question = input_data[i]['questions'][0]\n",
    "    answer = input_data[i]['answer'] + \"\\n\"\n",
    "    for k in range(len(input_data[i]['single_hops'])):\n",
    "        answer += input_data[i]['single_hops'][k]['cloze'] + \" \" + input_data[i]['single_hops'][k]['answer'] + \". \"\n",
    "    answer = answer.strip()\n",
    "\n",
    "    idx, data = data_append(data, 'mquake_cf', idx, None, question, answer)\n",
    "\n",
    "    # new_fact_unrelated\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.5:\n",
    "        out_of_scope_question = random.sample(out_of_scope_questions, 1)[0]\n",
    "        idx, data = data_append(data, 'mquake_cf', idx, new_facts, out_of_scope_question['question'], out_of_scope_question['answer'])\n",
    "        idx, data = data_append(data, 'mquake_cf', idx, None, out_of_scope_question['question'], out_of_scope_question['answer'])\n",
    "    else:\n",
    "        out_of_scope_question = random.sample(wizardlm_data, 1)[0]\n",
    "        assert out_of_scope_question['conversations'][0]['from'] == \"human\"\n",
    "        idx, data = data_append(data, 'mquake_cf', idx, new_facts, out_of_scope_question['conversations'][0]['value'], None)\n",
    "        idx, data = data_append(data, 'mquake_cf', idx, None, out_of_scope_question['conversations'][0]['value'], None)\n",
    "        need_gpt4.append({'question': out_of_scope_question['conversations'][0]['value'], 'prompt_new': out_of_scope_question['conversations'][0]['value']})\n",
    "\n",
    "\n",
    "### MQUAKE-T\n",
    "idx = 0\n",
    "with open(\"data/MQuAKE/MQuAKE-T.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "for i in range(1500):\n",
    "    # with edit\n",
    "    requested_rewrite = input_data[i]['requested_rewrite']\n",
    "    new_facts = \"\"\n",
    "    if len(requested_rewrite) > 1:\n",
    "        for j in range(len(requested_rewrite)):\n",
    "            new_facts += str(j+1) + \". \" + requested_rewrite[j]['prompt'].replace(\"{}\", requested_rewrite[j]['subject']) + \" \" + requested_rewrite[j]['target_new']['str'] + \"\\n\"\n",
    "    else:\n",
    "        new_facts += requested_rewrite[0]['prompt'].replace(\"{}\", requested_rewrite[0]['subject']) + \" \" + requested_rewrite[0]['target_new']['str']\n",
    "    new_facts = new_facts.strip(\"\\n\")\n",
    "    question = input_data[i]['questions'][0]\n",
    "\n",
    "    answer = input_data[i]['new_answer'] + \"\\n\"\n",
    "    for k in range(len(input_data[i]['new_single_hops'])):\n",
    "        answer += input_data[i]['new_single_hops'][k]['cloze'] + \" \" + input_data[i]['new_single_hops'][k]['answer'] + \". \"\n",
    "    answer = answer.strip()\n",
    "\n",
    "    idx, data = data_append(data, 'mquake_t', idx, new_facts, question, answer)\n",
    "    \n",
    "    # without edit\n",
    "    question = input_data[i]['questions'][0]\n",
    "    answer = input_data[i]['answer'] + \"\\n\"\n",
    "    for k in range(len(input_data[i]['single_hops'])):\n",
    "        answer += input_data[i]['single_hops'][k]['cloze'] + \" \" + input_data[i]['single_hops'][k]['answer'] + \". \"\n",
    "    answer = answer.strip()\n",
    "\n",
    "    idx, data = data_append(data, 'mquake_t', idx, None, question, answer)\n",
    "\n",
    "    # new_fact_unrelated\n",
    "    rand_num = random.random()\n",
    "    if rand_num < 0.5:\n",
    "        out_of_scope_question = random.sample(out_of_scope_questions, 1)[0]\n",
    "        idx, data = data_append(data, 'mquake_t', idx, new_facts, out_of_scope_question['question'], out_of_scope_question['answer'])\n",
    "        idx, data = data_append(data, 'mquake_t', idx, None, out_of_scope_question['question'], out_of_scope_question['answer'])\n",
    "    else:\n",
    "        out_of_scope_question = random.sample(wizardlm_data, 1)[0]\n",
    "        assert out_of_scope_question['conversations'][0]['from'] == \"human\"\n",
    "        idx, data = data_append(data, 'mquake_t', idx, new_facts, out_of_scope_question['conversations'][0]['value'], None)\n",
    "        idx, data = data_append(data, 'mquake_t', idx, None, out_of_scope_question['conversations'][0]['value'], None)\n",
    "        need_gpt4.append({'question': out_of_scope_question['conversations'][0]['value'], 'prompt_new': out_of_scope_question['conversations'][0]['value']})\n",
    "\n",
    "\n",
    "### COUNTERFACT related question\n",
    "idx = 0\n",
    "with open(\"data/counterfact/counterfact_related_QA.json\", 'r', encoding='utf-8') as input_file:\n",
    "    input_data = json.load(input_file)\n",
    "\n",
    "with open(\"data/WizardLM/WizardLM_evol_instruct_V2_143k.json\", 'r', encoding='utf-8') as input_file:\n",
    "    wizardlm_data = json.load(input_file)\n",
    "wizardlm_data = random.sample(wizardlm_data, len(wizardlm_data))\n",
    "\n",
    "for i in range(7500):\n",
    "    new_facts = input_data[i]['new_fact']\n",
    "    question = input_data[i]['question']\n",
    "    answer = input_data[i]['answer']\n",
    "    idx, data = data_append(data, 'counterfact_qa', idx, new_facts, question, answer)\n",
    "    idx, data = data_append(data, 'counterfact_qa', idx, None, question, None)\n",
    "    need_gpt4.append({'question': question, 'prompt_new': question_answering_prompt(question)})\n",
    "\n",
    "    question = wizardlm_data[i]['conversations'][0]['value']\n",
    "    assert wizardlm_data[i]['conversations'][0]['from'] == 'human'\n",
    "    idx, data = data_append(data, 'counterfact_qa', idx, new_facts, question, None)\n",
    "    idx, data = data_append(data, 'counterfact_qa', idx, None, question, None)\n",
    "    need_gpt4.append({'question': question, 'prompt_new': question})\n",
    "\n",
    "\n",
    "### save data\n",
    "json_str = json.dumps(data, indent=4)\n",
    "with open(\"LTE_train_data.json\", mode='w', encoding='utf-8') as output_file:\n",
    "    output_file.write(json_str)\n",
    "print(len(data))\n",
    "\n",
    "with open(\"LTE_train_data_need_gpt4.jsonl\", 'w', encoding='utf-8') as output_file:\n",
    "    for i in range(len(need_gpt4)):\n",
    "        output_file.write(json.dumps(need_gpt4[i])+ \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"LTE_train_data_need_gpt4.jsonl\" contains all data that requires GPT-4's completion. After acquiring GPT-4's answer, you should fill these answers into \"LTE_train_data.json\" to obtain the final training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPLearning_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
